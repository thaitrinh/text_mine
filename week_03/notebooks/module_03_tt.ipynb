{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Some of the cells in this notebook are computationally expensive. To reduce runtime, this notebook is using a subset of the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "\n",
    "# Sample the data to speed up computation\n",
    "# Comment out this line to match with lecture\n",
    "# df = df.sample(frac=0.1, random_state=10)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>1</td>\n",
       "      <td>I already had a phone with problems... I know ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>The charging port was loose. I got that solder...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>Phone looks good but wouldn't stay charged, ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I originally was using the Samsung S2 Galaxy f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great product it came after two days...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Product Name Brand Name   Price  \\\n",
       "0   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "5   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "6   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "7   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "8   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "11  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "    Rating                                            Reviews  Review Votes  \\\n",
       "0        5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1        4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "2        5                                       Very pleased           0.0   \n",
       "3        4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4        4  Great phone to replace my lost phone. The only...           0.0   \n",
       "5        1  I already had a phone with problems... I know ...           1.0   \n",
       "6        2  The charging port was loose. I got that solder...           0.0   \n",
       "7        2  Phone looks good but wouldn't stay charged, ha...           0.0   \n",
       "8        5  I originally was using the Samsung S2 Galaxy f...           0.0   \n",
       "11       5  This is a great product it came after two days...           0.0   \n",
       "\n",
       "    Positively Rated  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "5                  0  \n",
       "6                  0  \n",
       "7                  0  \n",
       "8                  1  \n",
       "11                 1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any 'neutral' ratings equal to 3\n",
    "df = df[df['Rating'] != 3]\n",
    "\n",
    "# Encode 4s and 5s as 1 (rated positively)\n",
    "# Encode 1s and 2s as 0 (rated poorly)\n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7482686025879323"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most ratings are positive\n",
    "df['Positively Rated'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], \n",
    "                                                    df['Positively Rated'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n",
      "\n",
      " I bought a BB Black and was deliveried a White BB.Really is not a serious provider...Next time is better to cancel the order.\n",
      "\n",
      "\n",
      "X_train shape:  (231207,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', X_train.iloc[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "\n",
    " - First approach: bag of word model. This ignore the structure, just focus on how often the words appear\n",
    " - It convert text into a matrix of token counts (numeric data)\n",
    " - After that sklearn can work with these (numeric) data\n",
    " - what CountVectorizer().fit does: tokenize the text to tokens, normalize all of them to lower case, use them to build a vocabulary\n",
    " - then, vect.transform() will transform the text X_train into bag-of-word representation (document matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '4less',\n",
       " 'adr6275',\n",
       " 'assignment',\n",
       " 'blazingly',\n",
       " 'cassettes',\n",
       " 'condishion',\n",
       " 'debi',\n",
       " 'dollarsshipping',\n",
       " 'esteem',\n",
       " 'flashy',\n",
       " 'gorila',\n",
       " 'human',\n",
       " 'irullu',\n",
       " 'like',\n",
       " 'microsaudered',\n",
       " 'nightmarish',\n",
       " 'p770',\n",
       " 'poori',\n",
       " 'quirky',\n",
       " 'responseive',\n",
       " 'send',\n",
       " 'sos',\n",
       " 'synch',\n",
       " 'trace',\n",
       " 'utiles',\n",
       " 'withstanding']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vocabulary\n",
    "vect.get_feature_names()[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53216"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count how many features/tokens do we have\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The transformed document is stored in SciPy sparse matrix. Each row is a document (a single review); each column is a word in our trained vocabulary\n",
    " - Each entry in the matrix = how many time the word occur in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<231207x53216 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6117776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9267064844136235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['worst' 'false' 'worthless' 'mony' 'junk' 'garbage' 'useless' 'messing'\n",
      " 'blacklist' 'unusable']\n",
      "\n",
      "Largest Coefs: \n",
      "['excelent' 'excelente' 'exelente' 'excellent' 'loving' 'loves'\n",
      " 'efficient' 'perfecto' 'amazing' 'lovely']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf\n",
    " - Term Frequency-Inverse Document Frequency\n",
    " - weight term based on how important they are in the document\n",
    " - high weight: for terms appear often in a particular document, but do not appear often in the whole corpus \n",
    " - low weight: terms which commonly used the in the whole corpus\n",
    " - low weight: terms rarely used or only appear in very long document\n",
    " - TfidfVectorizer().fit: tokenize the text, build vocabulary from tokens\n",
    " - then vect.transform() convert the data into matrix representation, which can be used in ML method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17951"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9266100666746837\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding in details TF-IDF\n",
    "\n",
    "https://stackoverflow.com/questions/36966019/how-aretf-idf-calculated-by-the-scikit-learn-tfidfvectorizer\n",
    "\n",
    "https://stackoverflow.com/questions/45680421/is-this-correct-tfidf\n",
    "\n",
    "Please ignore the link below, because it is not wrong but doesn't take into account the smoothing (+1) which is in the end not explain the case\n",
    "\n",
    "https://datascience.stackexchange.com/questions/15311/weights-for-keywords-in-a-set-of-documents-using-term-frequency-and-inverse-docu\n",
    "\n",
    "TF - IDF stands for  term frequency–inverse document frequency\n",
    "TF counts the frequency of a term / total #terms in a given document. For each term in a document, this value changes.\n",
    "IDF counts the log of ratio of total document / term appearing in #documents . This value is constant for a given unique term. Greater the idf value for a term, higher its significance.\n",
    "Example:\n",
    "\n",
    "Document 1: This is a sample example.\n",
    "\n",
    "Document 2: This is another example.\n",
    "\n",
    "__Calculate the TF__\n",
    " - TF(is, Document 1) = 1\n",
    " - TF(is, Document 2) = 1\n",
    " \n",
    "__Calculate the IDF__:\n",
    "The IDF for term = \"is\" (with smoothing) = log(2+1/2+1) + 1= 1\n",
    "Similarly the IDFs for 'this', 'example' are also 1\n",
    "The IDF for sample and another is: log(2+1/1+1) + 1 =  1.40546\n",
    "\n",
    "__Calculate the TF-IDF for document 1__: TFIDF = TF * IDF\n",
    " - TFIDF(is, document 1) = 1 * 1 = 1\n",
    " - Similarly, the TFIDF for \"this\", \"example\" in document 1 = 1\n",
    " - The TFIDF for \"sample\" = 1 * 1.40546 = 1.40546\n",
    " - The ITFIDF vector for document 1 is [1 (this), 1(is), 1.40546(sample), 1(example)]\n",
    "__Last step: normalize the TF-IDF vector:__\n",
    " - devide each element of the vector by (1^2 + 1^2 + 1.40546^2 + 1^2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    this is a sample example\n",
      "1     this is another example\n",
      "Name: 0, dtype: object\n",
      "{'this': 4, 'is': 2, 'sample': 3, 'example': 1, 'another': 0}\n",
      "[1.40546511 1.         1.         1.40546511 1.        ]\n",
      "['another', 'example', 'is', 'sample', 'this']\n",
      "  (0, 4)\t0.44832087319911734\n",
      "  (0, 2)\t0.44832087319911734\n",
      "  (0, 3)\t0.6300993445179441\n",
      "  (0, 1)\t0.44832087319911734\n",
      "  (1, 4)\t0.44832087319911734\n",
      "  (1, 2)\t0.44832087319911734\n",
      "  (1, 1)\t0.44832087319911734\n",
      "  (1, 0)\t0.6300993445179441\n",
      "1.4054651081081644\n",
      "0.44832087295952644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "d = pd.Series(['this is a sample example','this is another example'])\n",
    "df = pd.DataFrame(d)\n",
    "# tfidf_vectorizer = TfidfVectorizer(analyzer='word', min_df=0)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# if you want, say only top 2 features(terms)\n",
    "# tfidf_vectorizer = TfidfVectorizer(analyzer='word', min_df=0, max_features=2, max_df = 3)\n",
    "# Terms with given below:\n",
    "# occurred in too many documents (max_df, tfidf score = 3)  \n",
    "# occurred in too few documents (min_df, tfidf score = 0)\n",
    "# cut off by feature selection (max_features, tfidf score = 2).\n",
    "print(df[0])\n",
    "tfidf = tfidf_vectorizer.fit_transform(df[0])\n",
    "# dictionary: key is the words (feature names) and value is the index of the feature name is in the feature list\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "# output: {u'this': 4, u'sample': 3, u'is': 2, u'example': 1, u'another': 0}\n",
    "print(tfidf_vectorizer.idf_)\n",
    "# output(constant): [ 1.40546511  1.          1.          1.40546511  1.        ]\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print(tfidf)\n",
    "\n",
    "# Document 1: this: 1+1/4+1 * log(2/2)\n",
    "print(np.log(3/2) +1)\n",
    "print(1/np.sqrt(1**2 + 1**2 + 1.40546511**2 + 1**2))\n",
    "# output: \n",
    "#(0, 1)        0.448320873199    Document 1, term = example\n",
    "#(0, 3)        0.630099344518    Document 1, term = sample\n",
    "#(0, 2)        0.448320873199    Document 1, term = is\n",
    "#(0, 4)        0.448320873199    Document 1, term = this\n",
    "#(1, 0)        0.630099344518    Document 2, term = another\n",
    "#(1, 1)        0.448320873199    Document 2, term = example\n",
    "#(1, 2)        0.448320873199    Document 2, term = is\n",
    "#(1, 4)        0.448320873199    Document 2, term = this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find smalest and leargest tfidf \n",
    "__My understanding of the code below__: `X_train_vectorized` is a matrix. Rows are reviews, columns are word, each entry: how important a word appear in a document (weight). So `X_train_vectorized.max(0)` takes maximum in the `axis=0` (row axis). This means it give the maximum of each column. This means the maximum weight of each word across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.52778689  8.38822839 10.86616637 11.27163148 10.78612366]\n",
      "[0.71042189 0.32454897 0.31976905 0.26919504 0.21120331]\n",
      "Smallest tfidf:\n",
      "['commenter' 'pthalo' 'warmness' 'storageso' 'aggregration' '1300'\n",
      " '625nits' 'a10' 'submarket' 'brawns']\n",
      "\n",
      "Largest tfidf: \n",
      "['defective' 'batteries' 'gooood' 'epic' 'luis' 'goood' 'basico'\n",
      " 'aceptable' 'problems' 'excellant']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "\n",
    "# print(X_train_vectorized.toarray().shape) # 231207 reviews x 17951 words\n",
    "# after take maximum value from all rows, we have array of 1 x 17951 words\n",
    "# print(X_train_vectorized.max(0).toarray().shape) # (1,17951)\n",
    "\n",
    "print(vect.idf_[:5])\n",
    "print(X_train_vectorized.max(0).toarray()[0][:5])\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not' 'worst' 'useless' 'disappointed' 'terrible' 'return' 'waste' 'poor'\n",
      " 'horrible' 'doesn']\n",
      "\n",
      "Largest Coefs: \n",
      "['love' 'great' 'excellent' 'perfect' 'amazing' 'awesome' 'perfectly'\n",
      " 'easy' 'best' 'loves']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are treated the same by our current model\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198917"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9671350730214722\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These reviews are now correctly identified\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.91629073 1.91629073 1.91629073 1.51082562 1.91629073\n",
      " 1.51082562]\n"
     ]
    }
   ],
   "source": [
    "text = ['This is a string','This is another string','TFIDF computation calculation','TfIDF is the product of TF and IDF']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "X_vovab = vectorizer.get_feature_names()\n",
    "X_mat = X.todense()\n",
    "X_idf = vectorizer.idf_\n",
    "print(X_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
